# =============================================================================
# Zier Alpha Configuration Example
# =============================================================================
# Copy this file to ~/.zier-alpha/config.toml and edit to customise your assistant.
# The live configuration is loaded from that location. All fields have defaults;
# you can override only what you need.
#
# This example demonstrates the full range of features with sensible defaults.
# Features that are experimental or optional are disabled or commented out.
#
# Values can be templated with environment variables using ${VAR_NAME} syntax.

# -----------------------------------------------------------------------------
# [agent]
# -----------------------------------------------------------------------------
# Core LLM agent settings.
[agent]
# Default model to use. Several formats are supported:
#   - Provider/model: "anthropic/claude-sonnet-4-5", "openai/gpt-4o", "ollama/llama3"
#   - CLI aliases: "claude-cli/opus", "claude-cli/sonnet"
#   - Short aliases: "opus", "sonnet", "gpt", "gpt-mini"
# Set to a valid model identifier that matches a configured provider below.
default_model = "openai/gpt-4o"

# Total context window size in tokens (model-dependent)
context_window = 128000

# Tokens reserved for system prompt and tool schemas (subtracted from available)
reserve_tokens = 8000

# Maximum tokens for LLM response (passed to provider)
max_tokens = 4096

# Session compaction strategy – what to do when a session grows too large.
[agent.compaction]
# Strategy: "models" = try fallback models, "truncate" = keep last N messages,
# "models_then_truncate" = try models first, then truncate.
strategy = "models_then_truncate"
fallback_models = ["gpt-3.5-turbo"]  # Models to try in order
keep_last = 10                        # Number of messages to keep when truncating

# -----------------------------------------------------------------------------
# [providers]
# -----------------------------------------------------------------------------
# API providers. Omit sections you don't use. API keys may be literal or env vars.
[providers.openai]
api_key = "${OPENAI_API_KEY}"
base_url = "https://api.openai.com/v1"

[providers.anthropic]
api_key = "${ANTHROPIC_API_KEY}"
base_url = "https://api.anthropic.com"

# Local Ollama instance
[providers.ollama]
endpoint = "http://localhost:11434"
model = "llama3"

# Claude CLI (uses the `claude` binary)
[providers.claude_cli]
command = "claude"
model = "opus"

# Example for a custom provider (e.g., LiteLLM, Groq, etc.)
# [providers.custom]
# api_key = "${CUSTOM_API_KEY}"
# base_url = "https://custom.example.com/v1"

# -----------------------------------------------------------------------------
# [models]
# -----------------------------------------------------------------------------
# Custom model definitions and aliases. This is where you can create your own
# model presets with fallback chains and special settings.
#
# [models.my-gpt4]
# provider = "openai"
# model = "gpt-4-turbo-preview"
# api_key_env = "OPENAI_API_KEY"
# fallback_models = ["gpt-4", "gpt-3.5-turbo"]
#
# [models.gpt4-fallback]
# extend = "my-gpt4"
# fallback_settings = { default = "deny", allow = ["429"], deny = ["5*"] }

# -----------------------------------------------------------------------------
# [heartbeat]
# -----------------------------------------------------------------------------
# Background task that runs at regular intervals and posts status/events.
# Used mainly with the daemon for health monitoring and notifications.
[heartbeat]
enabled = false               # Set to true to enable heartbeat
interval = "30m"              # s/m/h/d, e.g., "30m", "1h", "1h30m"

# Optional: restrict to certain hours
# [heartbeat.active_hours]
# start = "09:00"
# end = "22:00"

# Optional: timezone override (defaults to system local)
# timezone = "America/New_York"

# -----------------------------------------------------------------------------
# [memory]
# -----------------------------------------------------------------------------
# Long-term memory storage.
[memory]
# Workspace directory (cognitive files live here)
workspace = "~/.zier-alpha/workspace"

# Embedding provider: "none" (FTS only), "local" (fastembed), "openai", "gguf"
embedding_provider = "none"
embedding_model = "all-MiniLM-L6-v2"
embedding_cache_dir = "~/.cache/zier-alpha/models"

# Chunking parameters (approx tokens, 1 token ≈ 4 chars)
chunk_size = 400
chunk_overlap = 80

# Additional directories to index
# paths = [
#   { path = "~/Documents/notes", pattern = "**/*.md" },
#   { path = "~/projects", pattern = "**/*.txt" }
# ]

# Session memory behaviour (/new command)
session_max_messages = 15    # 0 = unlimited
session_max_chars = 0        # 0 = unlimited

# -----------------------------------------------------------------------------
# [server]
# -----------------------------------------------------------------------------
# HTTP server for API and OpenAI compatibility. Enabled by default for local use.
[server]
enabled = true
port = 31327
bind = "127.0.0.1"

# OpenAI-compatible proxy (separate port)
[server.openai_proxy]
enabled = true
port = 37777
bind = "127.0.0.1"

# Telegram integration (polling mode is easiest to set up)
telegram_mode = "polling"
owner_telegram_id = 123456789       # Replace with your Telegram user ID
telegram_bot_token = "YOUR_BOT_TOKEN"
telegram_poll_timeout = 30

# For webhook mode (requires HTTPS), uncomment and set these:
# telegram_mode = "webhook"
# telegram_secret_token = "random-secret"
# public_url = "https://your.domain"

# -----------------------------------------------------------------------------
# [logging]
# -----------------------------------------------------------------------------
[logging]
level = "info"
file = "~/.zier-alpha/logs/agent.log"
retention_days = 7
max_log_size_mb = 100

# -----------------------------------------------------------------------------
# [disk]
# -----------------------------------------------------------------------------
[disk]
monitor_interval = "10m"
min_free_percent = 5
session_retention_days = 30
max_log_size_mb = 100

# -----------------------------------------------------------------------------
# [tools]
# -----------------------------------------------------------------------------
[tools]
bash_timeout_ms = 30000
web_fetch_max_bytes = 10000
require_approval = ["bash", "write_file", "edit_file"]
tool_output_max_chars = 50000
log_injection_warnings = true
use_content_delimiters = true
allowed_builtin = ["*"]

# -----------------------------------------------------------------------------
# [vision]
# -----------------------------------------------------------------------------
# Image handling via fallback model (if main LLM has no vision)
[vision]
enabled = false                 # Set true to enable image processing
fallback_model = "gpt-4o"
fallback_prompt = "Transcribe text and describe details in this image."

# -----------------------------------------------------------------------------
# [workdir]
# -----------------------------------------------------------------------------
# Strategy for filesystem layout
[workdir]
strategy = "overlay"
# custom_prompt = "..."

# -----------------------------------------------------------------------------
# [extensions]
# -----------------------------------------------------------------------------
# Hive subagent orchestration – a major feature.
[extensions.hive]
enabled = true                  # Set false to disable delegation
agents_dir = "agents"           # Your agent definitions (Markdown files)
max_depth = 3
ipc_mode = "artifact"
default_model = ""              # Empty = inherit parent's model
timeout_seconds = 300
cleanup_temp_files = true

# MCP (Model Context Protocol) servers – optional integration points.
[extensions.mcp]
cache_dir = "~/.zier-alpha/cache/mcp"
idle_timeout_secs = 600
default_strategy = "hybrid"

# Example MCP server definition (uncomment and adjust to use):
# [extensions.mcp.servers.filesystem]
# name = "filesystem"
# command = "npx"
# args = ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"]
# env = { DEBUG = "*" }
# native_tools = ["list_directory", "read_file"]

# -----------------------------------------------------------------------------
# End of configuration
# =============================================================================
